{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flexible-glasgow",
   "metadata": {},
   "source": [
    "# ConvNets and Image Data\n",
    "\n",
    "Image data is one area where neural networks stand head and shoulder over other model types.\n",
    "\n",
    "This is because of their ability to do complex operations directly on image data.\n",
    "\n",
    "### Convolution\n",
    "\n",
    "In a CNN, the input is a tensor with shape \n",
    "\n",
    "`(number of images) x (image height) x (image width) x (input channels)`. \n",
    "\n",
    "After passing through a convolutional layer, the image becomes abstracted to a feature map, with shape \n",
    "\n",
    "`(number of images) x (feature map height) x (feature map width) x (feature map channels)`\n",
    "\n",
    "A convolutional layer within a neural network should have the following attributes:\n",
    "\n",
    "- Convolutional filters/kernels defined by a width and height (hyper-parameters).\n",
    "- The number of input channels and output channels (hyper-parameter).\n",
    "- The depth of the convolution kernel/filter (the input channels) must equal the number channels (depth) of the input feature map.\n",
    "- The hyperparameters of the convolution operation, like padding size and stride.\n",
    "\n",
    "Convolutional layers convolve the input and pass its result to the next layer. \n",
    "\n",
    "Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is impractical for images. It would require a very high number of neurons, even in a shallow architecture, due to the very large input sizes associated with images, where each pixel is a relevant variable. \n",
    "\n",
    "For instance, a fully connected layer for a (small) image of size 100 x 100 has 10,000 weights for each neuron in the second layer. Instead, convolution reduces the number of free parameters, allowing the network to be deeper. For example, regardless of image size, tiling 5 x 5 region, each with the same shared weights, requires only 25 learnable parameters. Using regularized weights over fewer parameters avoids the vanishing gradient and exploding gradient problems seen during backpropagation in traditional neural networks.\n",
    "\n",
    "This means that the network learns to optimize the filters or convolution kernels that in traditional algorithms are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage. \n",
    "\n",
    "### Pooling layers\n",
    "\n",
    "Pooling layers reduce the dimensions of the data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. \n",
    "\n",
    "This is a way to reduce the image size -- leaving less computation to be done.\n",
    "\n",
    "Local pooling combines small clusters, typically 2 x 2. Global pooling acts on all the neurons of the convolutional layer. \n",
    "\n",
    "**Max pooling** uses the maximum value of each cluster of neurons at the prior layer\n",
    "\n",
    "**Average pooling** instead uses the average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "powered-subject",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "num_classes = 10\n",
    "### NOTE: The input is 28x28x1\n",
    "### It's not a vector anymore, it's a matrix!\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inside-calcium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 19s 44ms/step - loss: 0.7537 - accuracy: 0.7677 - val_loss: 0.0877 - val_accuracy: 0.9748\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.1284 - accuracy: 0.9602 - val_loss: 0.0596 - val_accuracy: 0.9838\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.0904 - accuracy: 0.9721 - val_loss: 0.0480 - val_accuracy: 0.9868\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 19s 44ms/step - loss: 0.0721 - accuracy: 0.9775 - val_loss: 0.0443 - val_accuracy: 0.9880\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 18s 42ms/step - loss: 0.0632 - accuracy: 0.9807 - val_loss: 0.0391 - val_accuracy: 0.9900\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 18s 42ms/step - loss: 0.0551 - accuracy: 0.9829 - val_loss: 0.0363 - val_accuracy: 0.9903\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 17s 41ms/step - loss: 0.0512 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9898\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.0463 - accuracy: 0.9853 - val_loss: 0.0328 - val_accuracy: 0.9907\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 18s 42ms/step - loss: 0.0433 - accuracy: 0.9861 - val_loss: 0.0343 - val_accuracy: 0.9902\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 19s 44ms/step - loss: 0.0423 - accuracy: 0.9858 - val_loss: 0.0342 - val_accuracy: 0.9913\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 19s 45ms/step - loss: 0.0403 - accuracy: 0.9881 - val_loss: 0.0301 - val_accuracy: 0.9917\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 18s 42ms/step - loss: 0.0369 - accuracy: 0.9880 - val_loss: 0.0294 - val_accuracy: 0.9912\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.0323 - accuracy: 0.9894 - val_loss: 0.0280 - val_accuracy: 0.9922\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 19s 44ms/step - loss: 0.0335 - accuracy: 0.9899 - val_loss: 0.0254 - val_accuracy: 0.9928\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.0291 - accuracy: 0.9909 - val_loss: 0.0261 - val_accuracy: 0.9925\n",
      "Test loss: 0.02434978447854519\n",
      "Test accuracy: 0.9908000230789185\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-island",
   "metadata": {},
   "source": [
    "# Pre-Trained Models\n",
    "\n",
    "Because Neural Nets are used with image and text data, it's common to pick up pre-trained models in those two domains (as we've already done with word embedding models). Here we'll use [VGG16](https://arxiv.org/abs/1409.1556) whose architecture is straightforward:\n",
    "\n",
    "![](vgg16.png)\n",
    "\n",
    "It's trained on the large [ImageNet](http://image-net.org/) dataset, however, so it's a good starting point for images in general and can classify images by types (imagenet is a labeled image classification dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "passive-jones",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doberman (35.42%)\n"
     ]
    }
   ],
   "source": [
    "# prepare an image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "# load model without output layer\n",
    "model = VGG16(include_top=False)\n",
    "\n",
    "# load model and specify a new input shape for images\n",
    "new_input = keras.Input(shape=(640, 480, 3))\n",
    "model = VGG16(include_top=False, input_tensor=new_input)\n",
    "\n",
    "#### LOAD IMAGES HERE\n",
    "# example of using a pre-trained model as a classifier\n",
    "# load an image from file\n",
    "image = load_img('dog.jpg', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "# load the model\n",
    "model = VGG16()\n",
    "# predict the probability across all output classes\n",
    "yhat = model.predict(image)\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "label = label[0][0]\n",
    "# print the classification\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-economics",
   "metadata": {},
   "source": [
    "# Fine Tuning a model\n",
    "\n",
    "By keeping all layers except the last one in a pre-trained model, we can \"fine-tune\" it to our purposes.\n",
    "\n",
    "![](finetune.jpg)\n",
    "\n",
    "You can find an example of this [here](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-demand",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
